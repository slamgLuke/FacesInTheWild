








import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler

import torchvision
from torchvision import transforms, datasets


# Para mac silicon chip
device = torch.device('mps')
print(f"using device: {device}")





train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

# Split the 'image1_image2' column into two columns
train_df[['image1', 'image2']] = train_df['image1_image2'].str.split('_', expand=True)
test_df[['image1', 'image2']] = test_df['image1_image2'].str.split('_', expand=True)

# Drop the original 'image1_image2' column
train_df.drop(columns=['image1_image2'], inplace=True)
test_df.drop(columns=['image1_image2'], inplace=True)

# Reorder cols
train_df = train_df[['image1', 'image2', 'label']]

# Convert label image to number
# diff = 0
# same = 1
#train_df['label'] = pd.factorize(train_df['label'])[0]

#train_df.head()

train_df['label'] = train_df['label'].apply(lambda x: 0 if x == 'diff' else 1)

# Imprimir la cantidad de ceros y unos
label_counts = train_df['label'].value_counts()
print(label_counts)

# Generar un gráfico de barras
label_counts.plot(kind='bar', color=['blue', 'orange'])
plt.title('Frecuencia de etiquetas en el conjunto de entrenamiento')
plt.xlabel('Etiqueta')
plt.ylabel('Frecuencia')
plt.xticks(ticks=[0, 1], labels=['Same (1)', 'Diff (0)'])
plt.show()

train_df.head()





class ImageHandler:
    def __init__(self, image_path = "cropped_faces_2"):
        self.image_path = image_path

    def load_img(self, img_id):
        path = f"{self.image_path}/{img_id}.png"
        return Image.open(path)

    def plot_imgs(self, imgs, size = 3):
        img_number = imgs.shape[0]
        rows = cols = math.ceil(np.sqrt(img_number))
        fig = plt.figure(figsize=(rows*size, cols*size))
        for i in range(img_number):
            fig.add_subplot(rows, cols, i+1)
            plt.imshow(imgs[i])
            plt.axis('off')
        plt.show()

    def visualize_samples(self, loader, num_samples = 5, size = 3):
        rows = cols = math.ceil(np.sqrt(num_samples))
        fig = plt.figure(figsize = (rows*size, cols*size))
        for images1, images2, labels in loader:
            for i in range(num_samples):
                plt.subplot(rows, cols, i+1)
                image = images1[i].permute(1, 2, 0).cpu().numpy()  # Change (C, H, W) to (H, W, C) and convert to numpy array
                plt.imshow(image)
                plt.axis('off')
            break
        plt.show()


print('> Train data before image splitting and label encoder');
print(train_df.head())

print('\n> Test data before image splitting and label encoder');
print(test_df.head())


ih = ImageHandler()

im1 = ih.load_img(train_df['image1'][1347])
im2 = ih.load_img(train_df['image2'][1347])

ih.plot_imgs(np.array([im1, im2]))
np.array(im1).shape








class TwoImageDataset(Dataset):
    def __init__(self, df, transform=None, is_test=False):
        self.df = df
        self.transform = transform
        self.is_test = is_test

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img1_id = self.df.iloc[idx, 0]
        img2_id = self.df.iloc[idx, 1]

        img1 = ih.load_img(img1_id).convert('RGB')
        img2 = ih.load_img(img2_id).convert('RGB')
        
        if self.transform:
            img1 = self.transform(img1)
            img2 = self.transform(img2)

        if self.is_test:
            return img1, img2
            
        label = self.df.iloc[idx, 2]
        return img1, img2, torch.tensor(label, dtype=torch.float32)





import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import transforms
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import pandas as pd
from torchvision.utils import make_grid
from torchvision import transforms

batch_size = 64

# Definir las transformaciones de augmentación de datos
data_augmentation_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.GaussianBlur(kernel_size=3,sigma=(0.1, 1.0)),
    transforms.ColorJitter(brightness=0.3, contrast=1, saturation=0.2),
    transforms.ToTensor(),  # Convert the image to a PyTorch tensor
])


# Define validation/test transformations
val_test_transforms = transforms.Compose([
    transforms.ToTensor(),
])

# Train-Test split
train_subset_df, val_subset_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=12)


# Initializing custom datasets with transformations
train_dataset = TwoImageDataset(train_subset_df, transform=data_augmentation_transforms)
val_dataset = TwoImageDataset(val_subset_df, transform=val_test_transforms)
test_dataset = TwoImageDataset(test_df, transform=val_test_transforms, is_test=True)

# Initialize the DataLoaders
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)



def show_dataset(dataset, n=10):

    imgs = torch.stack([dataset[i][0] for _ in range(n)
                       for i in range(9)])
    grid = make_grid(imgs).numpy()
    plt.imshow(np.transpose(grid, (1, 2, 0)), interpolation='nearest')
    plt.axis('off')

show_dataset(train_dataset)







from facenet_pytorch import InceptionResnetV1


# Create an inception resnet (in eval mode):
resnet = InceptionResnetV1(pretrained='vggface2').eval()
#print(nn.Sequential(*list(resnet.children())[:]))



from facenet_pytorch import InceptionResnetV1

class SNN(nn.Module):
    def __init__(self):
        super(SNN, self).__init__()
        self.resnet = InceptionResnetV1(pretrained='vggface2').eval()  # Usar el modelo preentrenado
        self.conv1 = nn.Sequential(*list(resnet.children())[:])
        
        # Congelar todas las capas del modelo Facenet
        for param in self.resnet.parameters():
            param.requires_grad = False
        for param in resnet.block8.parameters():
            param.requires_grad = True
            
        # Capas completamente conectadas después de la extracción de características
        # Salida de 512 por imagen
        self.fc = nn.Sequential(
            nn.Linear(512 * 2, 1024),  # Ajustar el tamaño de entrada
            nn.BatchNorm1d(1024),
            nn.LeakyReLU(negative_slope=0.01),
            nn.Linear(1024, 1024),
            nn.BatchNorm1d(1024),
            nn.LeakyReLU(negative_slope=0.01),
            nn.Linear(1024, 1),
            nn.Sigmoid()
        )

    def forward_once(self, x):
        embeddings = self.resnet(x)
        return embeddings

    def forward(self, img1, img2):
        out1 = self.forward_once(img1)
        out2 = self.forward_once(img2)
        concat = torch.cat((out1, out2), 1)
        output = self.fc(concat)
        return output

    def train_cnn(self, train_loader, val_loader, loss_fn, optimizer, scheduler=None, num_epochs=5, patience=5):
        total_step = len(train_loader)
        list_loss = []
        best_loss = float('inf')
        epochs_no_improve = 0
        
        for epoch in range(num_epochs):
            self.train()
            epoch_loss = 0.0
            
            for i, (images1, images2, labels) in enumerate(train_loader):
                images1 = images1.to(device)
                images2 = images2.to(device)
                labels = labels.to(device).unsqueeze(1) 
                
                output = self(images1, images2)
                loss = loss_fn(output, labels)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                list_loss.append(loss.item())

                if (i + 1) % 100 == 0:
                    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_step}], Loss: {loss.item():.4f}')
                
            avg_train_loss = epoch_loss / total_step
            avg_val_loss = self.evaluate(val_loader, loss_fn)
            
            scheduler.step()
            print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')
            if avg_val_loss < best_loss:
                best_loss = avg_val_loss
                epochs_no_improve = 0
                torch.save(self.state_dict(), 'best_model.pth')
            else:
                epochs_no_improve += 1
                if epochs_no_improve >= patience:
                    print('Early stopping triggered.')
                    break
        print('Finished Training Trainset')

        return list_loss
    def evaluate(self, val_loader, loss_fn):
        y_pred = []
        running_loss = 0.0
        self.eval()
        with torch.no_grad():
            for image1, image2, labels in val_loader:
                image1, image2, labels = image1.to(device), image2.to(device), labels.to(device).unsqueeze(1)
                output = self(image1, image2)
                loss = loss_fn(output, labels)
                running_loss += loss.item()
                output = output.cpu()
                pred = torch.round(output * 1000) / 1000 
                y_pred.append(float(pred.item()))
        avg_val_loss = running_loss / len(val_loader)
        return avg_val_loss
    def predict(self, dataloader, with_labels=False):
        y_pred = []
        self.eval()
        with torch.no_grad():
            if not with_labels:
                for image1, image2 in dataloader:
                    image1, image2 = image1.to(device), image2.to(device)
                    output = self(image1, image2)
                    output = output.cpu()
                    pred = torch.round(output)
                    y_pred.append(int(pred.item()))
            else:
                for image1, image2, _ in dataloader:
                    image1, image2 = image1.to(device), image2.to(device)
                    output = self(image1, image2)
                    output = output.cpu()
                    pred = torch.round(output)
                    y_pred.append(int(pred.item()))
        return y_pred





torch.mps.empty_cache()
learning_rate = 1e-4
model1 = SNN().to(device)

loss_fn = nn.BCELoss()

optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)

scheduler = StepLR(optimizer=optimizer, step_size=20, gamma=0.1)


torch.mps.empty_cache()
epochs = 50

# Entrenamiento del modelo0
list_loss = model1.train_cnn(train_dataloader,val_dataloader, loss_fn, optimizer, scheduler=scheduler, num_epochs=epochs)


# Plot the loss function
plt.figure(figsize=(10, 5))
plt.plot(list_loss, label='Training Loss', color='blue')
plt.title('Training Loss Over Iterations')
plt.xlabel('Iteration')

plt.ylabel('Loss')
plt.show()


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

val_pred = model1.predict(val_dataloader, with_labels=True)
cm = confusion_matrix(val_subset_df['label'], val_pred, normalize='true')

# Generate the classification report
report = classification_report(val_subset_df['label'], val_pred)
print("Classification Report:")
print(report)

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['different', 'same'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()





test_values = model1.predict(test_dataloader)
print(np.unique(test_values))


test_df_p4 = pd.read_csv("test.csv")  # Asegúrate de que tienes un archivo test.csv cargado

# Crear el DataFrame para la sumisión
submission_df = pd.DataFrame({
    'image1_image2': test_df_p4['image1_image2'],
    'label': ['same' if pred == 1 else 'diff' for pred in test_values]
})

# Guardar el DataFrame en un archivo CSV
submission_df.to_csv('submission_p4.csv', index=False)

print("Submission saved to submission_p4.csv")


# Guardar pesos del Mejor modelo
#torch.save(model1.state_dict(), 'snn_model.pth') -> pesa más de 200mb por eso no se puede guardar en github =c



